{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecoyComp: Dataset Comparison Executive Summary\n",
    "\n",
    "**Purpose**: Quick comparison of molecular benchmark datasets for virtual screening\n",
    "\n",
    "**Datasets Analyzed**: LIT-PCBA, DUDE-Z, DEKOIS2, MUV, D-COID\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have generated the dataset summaries:\n",
    "\n",
    "```bash\n",
    "# Generate file-based dataset summaries\n",
    "python analyse_datasets.py --roots LIT-PCBA DUDE-Z DEKOIS2 D-COID --include-muv --outdir results\n",
    "\n",
    "# Generate visualizations (optional, can be done in notebook)\n",
    "python metrics.py --smiles-dir results/smiles --outdir results --summary-csv results/dataset_unique_summary_split.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Imports\nimport polars as pl\nimport pandas as pd  # Only for matplotlib/seaborn compatibility\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom rdkit import Chem\nfrom rdkit.Chem import Descriptors, rdMolDescriptors\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import functions from metrics.py\nfrom metrics import COLOR_ACTIVE, COLOR_INACTIVE, violin_plot, compliance_bar_from_summary\n\n# Set style\nsns.set_style('whitegrid')\nplt.rcParams['figure.dpi'] = 100\nplt.rcParams['savefig.dpi'] = 300\nplt.rcParams['figure.figsize'] = (10, 6)\n\n# Configuration\nRESULTS_DIR = Path('results')\nSMILES_DIR = RESULTS_DIR / 'smiles'\nOUTPUT_DIR = Path('notebook_figures')\nOUTPUT_DIR.mkdir(exist_ok=True)\n\n# Dataset colors (consistent throughout notebook)\nDATASET_COLORS = {\n    'LIT-PCBA': '#1f77b4',\n    'DUDE-Z': '#ff7f0e',\n    'DEKOIS2': '#2ca02c',\n    'MUV': '#d62728',\n    'D-COID': '#9467bd'\n}\n\nprint(\"‚úì Setup complete (using Polars for data processing)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load pre-computed summaries using Polars\nsummary_file = RESULTS_DIR / 'dataset_unique_summary.csv'\nsplit_summary_file = RESULTS_DIR / 'dataset_unique_summary_split.csv'\n\nif not summary_file.exists():\n    raise FileNotFoundError(\n        f\"Summary file not found: {summary_file}\\n\"\n        \"Please run: python analyse_datasets.py --roots LIT-PCBA DUDE-Z DEKOIS2 D-COID --include-muv --outdir results\"\n    )\n\n# Load data with Polars (much faster than pandas)\ndf_summary_pl = pl.read_csv(summary_file)\ndf_split_pl = pl.read_csv(split_summary_file) if split_summary_file.exists() else None\n\nprint(f\"‚úì Loaded summary data (Polars):\")\nprint(f\"  - Overall summary: {df_summary_pl.shape}\")\nif df_split_pl is not None:\n    print(f\"  - Split summary: {df_split_pl.shape}\")\n\n# Display available datasets\ndatasets = df_summary_pl.get_column('Dataset').unique().to_list() if 'Dataset' in df_summary_pl.columns else []\nprint(f\"\\n‚úì Datasets found: {', '.join(datasets)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quick Overview: Dataset Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create comprehensive comparison table using Polars\ndef create_comparison_table(df_summary_pl: pl.DataFrame) -> pd.DataFrame:\n    \"\"\"Create a summary comparison table for all datasets.\"\"\"\n    \n    comparison = []\n    \n    for dataset in df_summary_pl.get_column('Dataset').unique().to_list():\n        # Get overall stats using Polars filtering\n        overall = df_summary_pl.filter(pl.col('Dataset') == dataset).row(0, named=True)\n        \n        # Get actives and inactives counts from overall summary\n        n_actives = int(overall.get('NumberActivesUnique', 0))\n        n_inactives = int(overall.get('NumberInactivesUnique', 0))\n        total_mols = overall['NumberLigandsUnique']\n        \n        # Calculate quality score (100 - percentage of problematic molecules)\n        invalid_pct = (overall.get('NumberInvalidSMILES', 0) / total_mols * 100) if total_mols > 0 else 0\n        quality_score = 100 - invalid_pct\n        \n        # Drug-likeness\n        lipinski = overall.get('LipinskiComplianceRate', 0) * 100\n        veber = overall.get('VeberComplianceRate', 0) * 100\n        \n        comparison.append({\n            'Dataset': dataset,\n            'Total Molecules': f\"{int(total_mols):,}\",\n            'Actives': f\"{n_actives:,}\",\n            'Decoys/Inactives': f\"{n_inactives:,}\",\n            'Active:Decoy Ratio': f\"1:{n_inactives/n_actives:.1f}\" if n_actives > 0 else 'N/A',\n            'Quality Score': f\"{quality_score:.1f}%\",\n            'Lipinski Compliant': f\"{lipinski:.1f}%\",\n            'Veber Compliant': f\"{veber:.1f}%\"\n        })\n    \n    # Return pandas DataFrame for display styling\n    return pd.DataFrame(comparison)\n\n# Create and display comparison table\ncomparison_df = create_comparison_table(df_summary_pl)\n\n# Style the table\nstyled_table = comparison_df.style.set_properties(**{\n    'text-align': 'left',\n    'font-size': '11pt'\n}).set_table_styles([{\n    'selector': 'th',\n    'props': [('font-size', '12pt'), ('text-align', 'center'), ('font-weight', 'bold')]\n}])\n\ndisplay(styled_table)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visual Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Dataset Size Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Bar chart: Total molecules and actives/decoys breakdown\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Extract data from Polars DataFrame\ndatasets = df_summary_pl.get_column('Dataset').to_list()\ntotal_mols = df_summary_pl.get_column('NumberLigandsUnique').to_list()\ncolors = [DATASET_COLORS.get(ds, 'gray') for ds in datasets]\n\nax1.bar(datasets, total_mols, color=colors, alpha=0.7)\nax1.set_ylabel('Number of Unique Molecules')\nax1.set_title('Dataset Sizes', fontweight='bold', fontsize=12)\nax1.tick_params(axis='x', rotation=45)\n\n# Add value labels on bars\nfor i, (ds, val) in enumerate(zip(datasets, total_mols)):\n    ax1.text(i, val, f'{int(val):,}', ha='center', va='bottom', fontsize=9)\n\n# Actives vs Decoys breakdown\nactives_counts = df_summary_pl.get_column('NumberActivesUnique').to_list()\ndecoys_counts = df_summary_pl.get_column('NumberInactivesUnique').to_list()\n\nx = np.arange(len(datasets))\nwidth = 0.35\n\nax2.bar(x - width/2, actives_counts, width, label='Actives', color=COLOR_ACTIVE, alpha=0.7)\nax2.bar(x + width/2, decoys_counts, width, label='Decoys/Inactives', color=COLOR_INACTIVE, alpha=0.7)\n\nax2.set_ylabel('Number of Molecules')\nax2.set_title('Active vs Decoy Distribution', fontweight='bold', fontsize=12)\nax2.set_xticks(x)\nax2.set_xticklabels(datasets, rotation=45, ha='right')\nax2.legend()\n\nplt.tight_layout()\nplt.savefig(OUTPUT_DIR / 'dataset_sizes.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"‚úì Dataset size comparison complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Chemical Property Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load or compute molecular descriptors using Polars\ndescriptors_file = RESULTS_DIR / 'ligand_descriptors.csv'\n\nif descriptors_file.exists():\n    print(f\"Loading descriptors from {descriptors_file} (Polars)...\")\n    df_desc_pl = pl.read_csv(descriptors_file)\n    # Convert to pandas for plotting compatibility with metrics.py functions\n    df_desc = df_desc_pl.to_pandas()\n    print(f\"‚úì Loaded {len(df_desc):,} molecular descriptors\")\nelse:\n    print(\"‚ö† Descriptor file not found. Run metrics.py first or compute descriptors below.\")\n    print(\"  python metrics.py --smiles-dir results/smiles --outdir results\")\n    df_desc_pl = None\n    df_desc = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create violin plots for key descriptors\n",
    "if df_desc is not None:\n",
    "    print(\"Generating molecular property distribution plots...\")\n",
    "    \n",
    "    # Use existing violin_plot function from metrics.py\n",
    "    violin_plot(df_desc, \"mw\", \"Molecular Weight (Da)\", \n",
    "                \"Molecular Weight Distribution\", OUTPUT_DIR / \"violin_mw.png\")\n",
    "    \n",
    "    violin_plot(df_desc, \"tpsa\", \"TPSA (≈≤)\", \n",
    "                \"Topological Polar Surface Area\", OUTPUT_DIR / \"violin_tpsa.png\")\n",
    "    \n",
    "    violin_plot(df_desc, \"rotbonds\", \"Rotatable Bonds\", \n",
    "                \"Rotatable Bonds Distribution\", OUTPUT_DIR / \"violin_rotbonds.png\")\n",
    "    \n",
    "    print(\"‚úì Violin plots saved to\", OUTPUT_DIR)\n",
    "    \n",
    "    # Display the MW plot inline\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(filename=OUTPUT_DIR / \"violin_mw.png\"))\n",
    "else:\n",
    "    print(\"‚ö† Skipping violin plots (descriptors not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Drug-likeness Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Drug-likeness compliance (Lipinski & Veber) using Polars data\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Extract data from Polars DataFrame\ndatasets = df_summary_pl.get_column('Dataset').to_list()\nlipinski_rates = [r * 100 for r in df_summary_pl.get_column('LipinskiComplianceRate').to_list()]\nveber_rates = [r * 100 for r in df_summary_pl.get_column('VeberComplianceRate').to_list()]\n\nx = np.arange(len(datasets))\nwidth = 0.35\n\nbars1 = ax.bar(x - width/2, lipinski_rates, width, label=\"Lipinski's Rule of Five\", \n               color='#3498db', alpha=0.8)\nbars2 = ax.bar(x + width/2, veber_rates, width, label=\"Veber's Rule\", \n               color='#e74c3c', alpha=0.8)\n\n# Add value labels\nfor bars in [bars1, bars2]:\n    for bar in bars:\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2., height,\n                f'{height:.1f}%', ha='center', va='bottom', fontsize=9)\n\nax.set_ylabel('Compliance Rate (%)', fontsize=11)\nax.set_title('Drug-likeness Rule Compliance', fontweight='bold', fontsize=13)\nax.set_xticks(x)\nax.set_xticklabels(datasets, rotation=45, ha='right')\nax.set_ylim(0, 105)\nax.legend(fontsize=10)\nax.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(OUTPUT_DIR / 'druglikeness_compliance.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"‚úì Drug-likeness compliance chart complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Data Quality Scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create quality metrics matrix using Polars\ndef create_quality_matrix(df_summary_pl: pl.DataFrame) -> pd.DataFrame:\n    \"\"\"Create a quality assessment matrix using Polars.\"\"\"\n    quality_data = []\n    \n    for ds in df_summary_pl.get_column('Dataset').to_list():\n        row = df_summary_pl.filter(pl.col('Dataset') == ds).row(0, named=True)\n        total = row['NumberLigandsUnique']\n        \n        # Calculate percentages\n        invalid_pct = (row.get('NumberInvalidSMILES', 0) / total * 100) if total > 0 else 0\n        valid_pct = 100 - invalid_pct\n        lipinski_pct = row.get('LipinskiComplianceRate', 0) * 100\n        veber_pct = row.get('VeberComplianceRate', 0) * 100\n        \n        quality_data.append({\n            'Dataset': ds,\n            'Valid SMILES': valid_pct,\n            'Lipinski Compliant': lipinski_pct,\n            'Veber Compliant': veber_pct\n        })\n    \n    return pd.DataFrame(quality_data)\n\nquality_df = create_quality_matrix(df_summary_pl)\n\n# Create heatmap\nfig, ax = plt.subplots(figsize=(10, 5))\n\n# Prepare data for heatmap (transpose so datasets are columns)\nheatmap_data = quality_df.set_index('Dataset').T\n\n# Create heatmap\nsns.heatmap(heatmap_data, annot=True, fmt='.1f', cmap='RdYlGn', \n            vmin=0, vmax=100, cbar_kws={'label': 'Percentage (%)'},\n            linewidths=0.5, ax=ax)\n\nax.set_title('Data Quality Scorecard', fontweight='bold', fontsize=13, pad=15)\nax.set_xlabel('Dataset', fontsize=11)\nax.set_ylabel('Quality Metric', fontsize=11)\n\nplt.tight_layout()\nplt.savefig(OUTPUT_DIR / 'quality_scorecard.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"‚úì Quality scorecard complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Chemical Space Visualization (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# PCA visualization of chemical space using Polars\nif df_desc_pl is not None:\n    print(\"Computing PCA for chemical space visualization...\")\n    \n    # Sample if too large (for performance) using Polars\n    max_samples_per_dataset = 5000\n    df_sampled_pl = (\n        df_desc_pl\n        .group_by('dataset')\n        .map_groups(lambda grp: grp.sample(n=min(len(grp), max_samples_per_dataset), seed=42))\n    )\n    \n    print(f\"  Using {len(df_sampled_pl):,} molecules for PCA (sampled from {len(df_desc_pl):,})\")\n    \n    # Extract numpy arrays for PCA\n    X = df_sampled_pl.select(['mw', 'tpsa', 'rotbonds']).to_numpy()\n    dataset_labels = df_sampled_pl.get_column('dataset').to_list()\n    \n    # Standardize and apply PCA\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X_scaled)\n    \n    # Create scatter plot\n    fig, ax = plt.subplots(figsize=(10, 8))\n    \n    unique_datasets = df_sampled_pl.get_column('dataset').unique().to_list()\n    for ds in unique_datasets:\n        mask = [d == ds for d in dataset_labels]\n        ax.scatter(X_pca[mask, 0], X_pca[mask, 1], \n                  label=ds, alpha=0.4, s=10,\n                  color=DATASET_COLORS.get(ds, 'gray'))\n    \n    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=11)\n    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=11)\n    ax.set_title('Chemical Space Distribution (PCA)', fontweight='bold', fontsize=13)\n    ax.legend(loc='best', framealpha=0.9)\n    ax.grid(alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(OUTPUT_DIR / 'chemical_space_pca.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    print(f\"‚úì PCA complete (explained variance: {pca.explained_variance_ratio_.sum()*100:.1f}%)\")\nelse:\n    print(\"‚ö† Skipping PCA (descriptors not available)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Key Findings & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Auto-generate key findings using Polars\ndef generate_key_findings(df_summary_pl: pl.DataFrame):\n    \"\"\"Automatically identify and report key findings using Polars.\"\"\"\n    findings = []\n    \n    # 1. Dataset sizes\n    largest_ds = df_summary_pl.sort('NumberLigandsUnique', descending=True).row(0, named=True)\n    smallest_ds = df_summary_pl.sort('NumberLigandsUnique', descending=False).row(0, named=True)\n    \n    findings.append(\n        f\"**Dataset Size**: {largest_ds['Dataset']} is the largest with \"\n        f\"{int(largest_ds['NumberLigandsUnique']):,} unique molecules, while \"\n        f\"{smallest_ds['Dataset']} is the smallest with {int(smallest_ds['NumberLigandsUnique']):,} molecules.\"\n    )\n    \n    # 2. Drug-likeness\n    best_lipinski = df_summary_pl.sort('LipinskiComplianceRate', descending=True).row(0, named=True)\n    findings.append(\n        f\"**Drug-likeness**: {best_lipinski['Dataset']} has the highest Lipinski compliance at \"\n        f\"{best_lipinski['LipinskiComplianceRate']*100:.1f}%, making it most suitable for drug-like compound screening.\"\n    )\n    \n    # 3. Data quality\n    df_with_invalid = df_summary_pl.with_columns(\n        (pl.col('NumberInvalidSMILES') / pl.col('NumberLigandsUnique')).alias('invalid_rate')\n    )\n    cleanest_ds = df_with_invalid.sort('invalid_rate', descending=False).row(0, named=True)\n    findings.append(\n        f\"**Data Quality**: {cleanest_ds['Dataset']} has the lowest rate of invalid SMILES \"\n        f\"({cleanest_ds['invalid_rate']*100:.3f}%), indicating high data quality.\"\n    )\n    \n    # 4. Active:Decoy ratios\n    ratios = []\n    for row in df_summary_pl.iter_rows(named=True):\n        n_actives = row['NumberActivesUnique']\n        n_inactives = row['NumberInactivesUnique']\n        if n_actives > 0:\n            ratios.append((row['Dataset'], n_inactives / n_actives))\n    \n    if ratios:\n        most_balanced = min(ratios, key=lambda x: abs(x[1] - 1))\n        findings.append(\n            f\"**Balance**: {most_balanced[0]} has the most balanced active:decoy ratio \"\n            f\"(1:{most_balanced[1]:.1f}), ideal for unbiased model training.\"\n        )\n    \n    return findings\n\nfindings = generate_key_findings(df_summary_pl)\n\nprint(\"## üìä Key Findings\\n\")\nfor i, finding in enumerate(findings, 1):\n    print(f\"{i}. {finding}\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Use case recommendations\nbest_lipinski_ds = df_summary_pl.sort('LipinskiComplianceRate', descending=True).row(0, named=True)['Dataset']\n\nrecommendations = {\n    'Method Development & Testing': [\n        'Use the cleanest dataset with high-quality SMILES',\n        'Recommended: Dataset with highest data quality score'\n    ],\n    'Realistic Benchmarking': [\n        'Use literature-derived datasets for real-world relevance',\n        'Recommended: LIT-PCBA (literature sources)'\n    ],\n    'Challenging Model Evaluation': [\n        'Use datasets with matched/sophisticated decoys',\n        'Recommended: DUDE-Z (property-matched decoys)'\n    ],\n    'Drug-like Compound Screening': [\n        'Use dataset with highest Lipinski compliance',\n        f'Recommended: {best_lipinski_ds}'\n    ],\n    'Structure-based Studies': [\n        'Use datasets with structural information',\n        'Recommended: D-COID (PDB-derived structures)'\n    ],\n    'Bioactivity Prediction': [\n        'Use datasets with experimental bioactivity data',\n        'Recommended: MUV (bioactivity database)'\n    ]\n}\n\nprint(\"## üéØ Use Case Recommendations\\n\")\nfor use_case, details in recommendations.items():\n    print(f\"**{use_case}**\")\n    for detail in details:\n        print(f\"  ‚Ä¢ {detail}\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary Statistics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute summary statistics for molecular descriptors using Polars\nif df_desc_pl is not None:\n    # Compute statistics using Polars (much faster than pandas groupby)\n    stats_summary_pl = df_desc_pl.group_by('dataset').agg([\n        pl.col('mw').mean().alias('mw_mean'),\n        pl.col('mw').std().alias('mw_std'),\n        pl.col('mw').median().alias('mw_median'),\n        pl.col('tpsa').mean().alias('tpsa_mean'),\n        pl.col('tpsa').std().alias('tpsa_std'),\n        pl.col('tpsa').median().alias('tpsa_median'),\n        pl.col('rotbonds').mean().alias('rotbonds_mean'),\n        pl.col('rotbonds').std().alias('rotbonds_std'),\n        pl.col('rotbonds').median().alias('rotbonds_median'),\n    ]).sort('dataset')\n    \n    # Round and convert to pandas for display\n    stats_summary = stats_summary_pl.to_pandas().set_index('dataset').round(2)\n    \n    print(\"## üìà Molecular Descriptor Statistics\\n\")\n    display(stats_summary)\n    \n    # Save to CSV using Polars\n    stats_summary_pl.write_csv(OUTPUT_DIR / 'descriptor_statistics.csv')\n    print(f\"\\n‚úì Statistics saved to {OUTPUT_DIR / 'descriptor_statistics.csv'}\")\nelse:\n    print(\"‚ö† Descriptor statistics not available (descriptors not loaded)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions\n",
    "\n",
    "### Summary\n",
    "\n",
    "This executive summary compared five benchmark molecular datasets for virtual screening:\n",
    "\n",
    "- **LIT-PCBA**: Literature-derived actives and inactives\n",
    "- **DUDE-Z**: Property-matched decoys for challenging evaluation\n",
    "- **DEKOIS2**: ZINC-derived decoys with diverse scaffolds\n",
    "- **MUV**: Bioactivity database with experimental data\n",
    "- **D-COID**: Structure-based dataset from PDB complexes\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Select appropriate dataset(s)** based on use case recommendations above\n",
    "2. **Consider combining datasets** for more robust validation\n",
    "3. **Validate findings** with your specific screening protocol\n",
    "4. **Consult comprehensive analysis** notebook for detailed statistics\n",
    "\n",
    "---\n",
    "\n",
    "**For detailed analysis**: See `dataset_comparison_comprehensive.ipynb` (when available)\n",
    "\n",
    "**Questions or feedback**: [Contact information]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of generated outputs\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìÅ OUTPUTS GENERATED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAll figures saved to: {OUTPUT_DIR.absolute()}\\n\")\n",
    "\n",
    "output_files = list(OUTPUT_DIR.glob('*.png')) + list(OUTPUT_DIR.glob('*.csv'))\n",
    "for f in sorted(output_files):\n",
    "    print(f\"  ‚úì {f.name}\")\n",
    "\n",
    "print(f\"\\nTotal files: {len(output_files)}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}